{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape página web Congreso Visible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  importar librerías que se van a usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  sacar el listado de las 125 página donde se encuentran los proyectos de Ley "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sufijo que  de las páginas que se deben visitar para extrar las url de cada proyecto \n",
    "proyectos = 'https://congresovisible.uniandes.edu.co/proyectos-de-ley/#q=2019&page='\n",
    " \n",
    "proyectos_list = []\n",
    "for i in [x for x in range(1,126)]:\n",
    "    proyectos_list.append(proyectos + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  una vez se tiene la lista con las 125 urls: se crea una función que permita extraer la url de cada proyecto de ley "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacar_links_pdl(lista):\n",
    "    browser = webdriver.Chrome(executable_path=r\"C:\\web_drivers\\chromedriver.exe\")\n",
    "    browser.get(lista)\n",
    "    time.sleep(2) # tiempo que tarda el java en cargar la info de la web \n",
    "    html = browser.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "    time.sleep(0)\n",
    "    soup = BeautifulSoup(html,'lxml') # crear soup \n",
    "    \n",
    "    elements = soup.find_all('a') # buscar todos los links \n",
    "   \n",
    "    pl_prefix = 'https://congresovisible.uniandes.edu.co/proyectos-de-ley/'\n",
    "    \n",
    "    pl_sufix = re.findall('/proyectos-de-ley/(p.+?)\\\">', str(elements)) # extraer links que coinciden con este inicio \n",
    "    \n",
    "    pl_all_list = [pl_prefix + url for url in pl_sufix] \n",
    "    \n",
    "    return pl_all_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### se aplica la función a la lista de url´s previamente creada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_proyectos_ley = list(map(sacar_links_pdl,proyectos_list)) # resultado: 625 urls, una para cada proyecto de ley "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algoritmo que extrae todos los elementos que se requieren de la pag web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_elementos(url):\n",
    "       \n",
    "    try: \n",
    "        get_url = requests.get(url).text\n",
    "        sopa = BeautifulSoup(get_url,'lxml')\n",
    "        \n",
    "        r_titulo = sopa.find_all('h1')\n",
    "        try:\n",
    "            titulo = re.findall('\\\">\"|“(.+?)\\[', str(r_titulo))[-1]\n",
    "        except:\n",
    "            titulo = titulo = re.findall('\\\">“(.+?)\\.', str(r_titulo))\n",
    "        \n",
    "        try:\n",
    "            tagtitulo = re.findall('\\[([A-Z\\d].+?)\\]', str(r_titulo))[-1]\n",
    "               \n",
    "        except:\n",
    "            tagtitulo = 'NA'\n",
    "        del r_titulo\n",
    "        \n",
    "        r_estado = sopa.find('div', class_='module5').find_all('li', text='')\n",
    "        estado = re.findall('\\\">(.+?)</a>', str(r_estado))[-1]\n",
    "        proceso = re.findall('\\\">(.+?)</a>', str(r_estado))\n",
    "        fechas_proceso = re.findall('<p>(.+?)</p>', str(r_estado))\n",
    "        del r_estado\n",
    "        \n",
    "        try:\n",
    "            r_sinopsis = sopa.find('div', class_='module7').find('p', text='')\n",
    "            sinopsis = re.findall('p>(.+?)</', str(r_sinopsis))\n",
    "            sinopsis = [w.replace('<br/>', ' ') for w in sinopsis][-1]\n",
    "            del r_sinopsis\n",
    "        except:\n",
    "            sinopsis = 'NA'\n",
    "        \n",
    "        r_datos_grls = sopa.find('ul', class_='lista6').find_all('li', text='')\n",
    "        str_datos_grls = str(r_datos_grls)\n",
    "        del r_datos_grls\n",
    "        \n",
    "        temas = re.findall('Tema</h3><p>(.+?)</p>', str_datos_grls)\n",
    "        temas = Na_taguer(temas)\n",
    "        tipo = re.findall('Tipo</h3><p>(.+?)</p>', str_datos_grls)[-1]\n",
    "        tipo = Na_taguer(tipo)\n",
    "        \n",
    "        fecha_radicacion = re.findall('Fecha de Radicación </h3><p>(.+?)</p>', str_datos_grls)[-1]\n",
    "        \n",
    "        tags = re.findall('#q=(.+?)\\\">', str_datos_grls)\n",
    "        tags = [x.strip(' ') for x in tags]\n",
    "        tags = Na_taguer(tags)\n",
    "        \n",
    "        r_autores = sopa.find('div', class_='module2')\n",
    "        autores = re.findall('alt=\\\"([A-ZÁÉÍÓÚ].+?)\\\" height', str(r_autores))\n",
    "        autores = Na_taguer(autores)\n",
    "        del r_autores\n",
    "        \n",
    "        r_partido_autores = sopa.find_all('div', class_='module2')\n",
    "        partido_autores1 = re.findall('\\d/\\\">(.+?)</p>', str(r_partido_autores))\n",
    "        partido_autores = re.findall('\\d/\\\">(.+?)</a>', str(partido_autores1))\n",
    "        partido_autores = Na_taguer(partido_autores)\n",
    "        del r_partido_autores\n",
    "        del partido_autores1\n",
    "        \n",
    "        url_Pdl = url\n",
    "    \n",
    "        return{'Proyecto de Ley':titulo,'Palabra Clave':tagtitulo,'Estado':estado,'Autor':autores,'Partido':partido_autores ,'Proceso':proceso,'Fechas proceso':fechas_proceso,'Resumen':sinopsis,'Tema':temas,'Tipo':tipo,'Fecha Radicación':fecha_radicacion,'Tags':tags,'link':url_Pdl}\n",
    "# se retorna un diccionario con los elementos requeridos \n",
    "    except:  # en caso de que algún link no funcione, para que no se detenga toda la aplciación por un error \n",
    "        \n",
    "        titulo = 'link dañado'\n",
    "        tagtitulo = 'link dañado'\n",
    "        estado = 'link dañado'\n",
    "        autores = 'link dañado'\n",
    "        partido_autores = 'link dañado'\n",
    "        proceso = 'link dañado'\n",
    "        fechas_proceso = 'link dañado'\n",
    "        sinopsis = 'link dañado'\n",
    "        temas = 'link dañado'\n",
    "        tipo = 'link dañado'\n",
    "        fecha_radicacion = 'link dañado'\n",
    "        tags = 'link dañado'\n",
    "        url_Pdl = url\n",
    "        \n",
    "        return{'Proyecto de Ley':titulo,'Palabra Clave':tagtitulo,'Estado':estado,'Autor':autores,'Partido':partido_autores,'Proceso':proceso,'Fechas proceso':fechas_proceso,'Resumen':sinopsis,'Tema':temas,'Tipo':tipo,'Fecha Radicación':fecha_radicacion,'Tags':tags,'link':url}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  se aplica la anterior función a cada url de cada uno de los 625 proyectos de Ley "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_Dicc = list(map(scrap_elementos,url_625_PdL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Se crea el DataFrame  de la MacroBase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_DF = pd.DataFrame(Macro_Dicc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
